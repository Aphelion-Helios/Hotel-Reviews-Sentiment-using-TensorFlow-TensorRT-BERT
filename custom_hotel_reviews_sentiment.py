# -*- coding: utf-8 -*-
"""Custom_Hotel_Reviews_sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nm4GVnpOZNtxrU6HbXVYz7f_hyzjHjy3

# Sentiment analysis of hotel reviews with BERT
Using some reviews I scraped and pulled through google translate for an Aspect Based Sentiment Analysis (ABSA) job, this notebook shows how to use a pretrained BERT model from Hugging Face to train a simple sentiment classifier using TensorFlow. The code contains all the necessary lines to use the full BERT model, but are commented out to use distilBERT instead, to speed up the finetuning job.

Looking for an approach using the simple fit command?
https://colab.research.google.com/drive/1mdjPYkBnJhMG3-5cPaLvvByER5ja-HHw?usp=sharing

## Instal these packages
"""

!pip install "tensorflow >=2.0"
!pip install transformers
!pip install nltk
!pip install -q tf-models-official==2.4.0

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import os
import datetime

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

import tensorflow as tf
from official import nlp
import official.nlp.optimization

from transformers import DistilBertTokenizer, TFDistilBertModel
from transformers import BertTokenizer, TFBertModel

from tensorflow.keras.layers import Dense, GlobalMaxPooling1D, Dropout, Input
from tensorflow.keras.models import Model

from tensorflow.data import AUTOTUNE

from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report
import seaborn as sn

print("Version: ", tf.__version__)
print("GPU is", "available" if tf.config.list_physical_devices('GPU') else "NOT AVAILABLE")
if tf.config.list_physical_devices('GPU'):
  print(tf.config.list_physical_devices('GPU'))

"""## I scraped a bunch of reviews from the web and placed them in my drive"""

from google.colab import drive
drive.mount('/content/drive')

directory = '/content/drive/My Drive/Sentiment/data/'
filename = 'reviews.csv'

df = pd.read_csv(directory + filename)

print(df.shape[0])
df.head()

df.isnull().values.any() # Check for NaN values in the dataframe

"""## We need to clean the data.
We're going to try some light cleaning, consisting of removing urls, ascii characters and lower casing the sentences, and some heavier cleaning, which adds the removal of stopwords using the NLTK stopwords.
"""

stop_words = set(stopwords.words('english'))

def PreProcessing(sentence, remove_stopwords=False):
  sentence = sentence.lower()
  sentence = re.sub("https*\S+", " ", sentence) # Remove urls
  sentence = sentence.encode('ascii', 'ignore').decode() # Remove ASCII characters
  #sentence = re.sub(r"\'t", " not", sentence) # Replace 't with not
  sentence = re.sub(r'([\^\#\@\;\:\|•«\n])', ' ', sentence) # remove some special characters
  sentence = re.sub('\s{2,}', " ", sentence) # Remove extra white spaces

  if remove_stopwords:
    words = sentence.split() # Split sentence into list of words
    words = [w for w in words if not w in stop_words] # Filter out stop words
    sentence = ' '.join(words)

  return sentence

df['review_cleaned'] = df.review.apply(PreProcessing)
df['review_cleaned_wo_stopwords'] = df.review.apply(lambda x: PreProcessing(x, remove_stopwords=True))
print(df.review_cleaned.head(5))
print(df.review_cleaned_wo_stopwords.head(5))

"""## Divide data into train and validation sets before upsampling to avoid data leakage
We use a fixed random state (seed) when sampling the data before splitting it into the train-test sets. This is because, when you get kicked of Google Colab and want to continue finetuning your model later on, the dataset has to be preprocessed again. Without a fixed random state, this will generate a different train-test set each time, causing data leakage.
"""

random_state = 5 # Set random state to be used when sampling data

split = 0.8 # Set fraction of train-valid set
df = df.sample(frac=1, random_state=random_state) # Shuffle the dataframe before sampling train and valid sets to ensure equal distribution
train = df[:round(split*len(df))].reset_index(drop=True)
valid = df[round(split*len(df)):].reset_index(drop=True)
print(train.rating.value_counts(normalize=True)) # Check distribution train set
print(valid.rating.value_counts(normalize=True)) # Check distribution valid set

"""## Check the division of the classes
We're going to re-order the classes into negative(0) (1-2 stars), neutral(1) (3 stars) and positive(2) (4-5 stars). Furthermore, we're going to upsample the minority classes so that we don't have to drop data from the dominant classes.
"""

train.rating.value_counts() # Groups reviews by rating and counts them

train.loc[:, 'rating_new'] = train.rating.replace(to_replace=(5, 4, 3, 2, 1), value=(2, 2, 1, 0, 0)).astype(int)
valid.loc[:, 'rating_new'] = valid.rating.replace(to_replace=(5, 4, 3, 2, 1), value=(2, 2, 1, 0, 0)).astype(int)

print(train.rating.head(5))
print(train.rating_new.head(5))
print(train.rating_new.value_counts())

class0 = train.review_cleaned[train.rating_new==0]
class1 = train.review_cleaned[train.rating_new==1]
class2 = train.review_cleaned[train.rating_new==2]

'''
As in this notebook a custom training loop is used which checkpoints the model
multiple times per epoch, the upsampling of underrepresented classes should be
done under a fixed random seed as well. Otherwise, when a checkpoint is loaded
from the middle of an epoch, and the train dataset is composed differently, some
data might be missed or seen too many times.
'''
rng = np.random.default_rng(random_state)

class0_upsampled = rng.choice(class0, size=len(class2), replace=True)
class1_upsampled = rng.choice(class1, size=len(class2), replace=True)
print(len(class0_upsampled))
print(len(class1_upsampled))

def make_df(reviews, target):
  df_temp = pd.DataFrame(data={'rating': target*np.ones(len(reviews), dtype=np.int8), 'review': reviews}) #, columns=['rating', 'review'])
  return df_temp

# Create dataframes with concurrent target for each class
df_class0 = make_df(class0_upsampled, 0)
df_class1 = make_df(class1_upsampled, 1)
df_class2 = make_df(class2, 2)

# Concatenate classes, shuffle them with a fixed random seed and reset the index
train_final = pd.concat([df_class0, df_class1, df_class2], axis=0).sample(frac=1, random_state=random_state).reset_index(drop=True)

train_final.head(10)

x_train = train_final.review
#y_train = pd.get_dummies(train_final.rating)
y_train = train_final.rating.values
x_valid = valid.review
#y_valid = pd.get_dummies(valid.rating_new)
y_valid = valid.rating_new.values
y_valid

"""## Choose max sentence length

"""

reviews_len = [len(review.split()) for review in x_train] # Get a list containing the lengths of all reviews

bins = np.arange(0, 1000, 50) # Bins for histogram
plt.figure()
plt.hist(reviews_len, bins=bins)
plt.xlabel('Number of words')
plt.ylabel('Number of reviews')
plt.grid(b=True, which='major', color='#666666', linestyle='-')
plt.minorticks_on()
plt.grid(b=True, which='minor', color='#999999', linestyle='-', alpha=0.2)

# The length of 94% of the reviews falls in 256 words. Do note that this does not
# mean that up to 256 words get inserted into the BERT model. This is because the
# tokenizer splits some words in multiple pieces, and also tokenizes interpunction.
percentage = len([review for review in reviews_len if review < 256]) / len(x_train)
percentage

"""## Get Tokenizer and tokenize data"""

#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
MAX_SEQUENCE_LENGTH = 256

testsentence = "Hi, this is a sentence to test the tokenizer. I'm curious what's about to happen!"

# As you can see, the tokenizer outputs more tokens than words present in the testsentence
encoded_input = tokenizer(testsentence, return_tensors='tf', max_length=MAX_SEQUENCE_LENGTH,
                          padding='max_length', add_special_tokens=True)
encoded_input

X_train = tokenizer(x_train.values.tolist(), return_tensors='tf', max_length=MAX_SEQUENCE_LENGTH,
                          padding='max_length', add_special_tokens=True, truncation=True)
X_valid = tokenizer(x_valid.values.tolist(), return_tensors='tf', max_length=MAX_SEQUENCE_LENGTH,
                          padding='max_length', add_special_tokens=True, truncation=True)

"""## Wrap the data in a tensorflow Dataset
We also cache it in memory, define batch size and prefetch data.
"""

BS = 6 # Batch size
'''
Don't shuffle data when using the custom training loop which checkpoints
multiple times per epoch, as this will change the training set every time google
kicks you off the vm
'''
#train_ds = tf.data.Dataset.from_tensor_slices((X_train.data, y_train)).shuffle(1024).cache().repeat().batch(BS).prefetch(AUTOTUNE)
train_ds = tf.data.Dataset.from_tensor_slices((X_train.data, y_train)).cache().repeat().batch(BS).prefetch(AUTOTUNE)
valid_ds = tf.data.Dataset.from_tensor_slices((X_valid.data, y_valid)).cache().batch(BS).prefetch(AUTOTUNE)
train_ds

"""## Create the model
Both BERT and distilBERT are implemented. Follow the instructions in the comments to decide which to use
"""

def create_model():
    '''
    The first token of every sequence is always a special classification token ([CLS]).
    The final hidden state corresponding to this token is used as the aggregate
    sequence representation for classification tasks. BERT paper: https://arxiv.org/pdf/1810.04805.pdf
    '''

    #BERT_model = TFBertModel.from_pretrained("bert-base-uncased") # BERT
    distilBERT_model = TFDistilBertModel.from_pretrained("distilbert-base-uncased") # distilBERT

    input_id = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_ids')
    input_mask = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='attention_mask')
    #token_type = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='token_type_ids') # Uncomment this when using BERT instead of distilBERT

    ## Uncomment this when using BERT instead of distilBERT
    #embedding = BERT_model([input_id, token_type, input_mask])[1] # BERT has 2 outputs, grab the pooled output, which is the [CLS] token output
    #x = Dropout(0.2)(embedding)
    #x = Dense(768, activation='relu')(embedding) # BERT
    #x = Dropout(0.2)(x) # BERT

    ## Uncomment this when using distilBERT instead of BERT
    embedding = distilBERT_model([input_id, input_mask])
    last_hidden_state_cls = embedding.last_hidden_state[:,0,:] # Select the last hidden state, and use the output embedding of the first token [CLS] for classification
    x = Dropout(0.2)(last_hidden_state_cls)
    x = Dense(768, activation='relu')(x)
    x = Dropout(0.2)(x)

    ## Softmax classification layer
    x = Dense(3, activation='softmax')(x)

    ## Uncomment when using BERT:
    #model = Model(inputs=[input_id, token_type, input_mask], outputs=x)

    ## Uncomment when using distilBERT:
    model = Model(inputs=[input_id, input_mask], outputs=x)

    return model

print('Creating model..')
model = create_model()

"""## Some visualizations of the model"""

tf.keras.utils.plot_model(model)

model.summary()

'''
If you don't want to train the BERT parameters, you can freeze the BERT layer.
This way only the added layers will be trained for the classification problem.
'''
## For distilBERT, freeze layer 2. For BERT, freeze layer 3. You can also print
## the summary again to see the new number of trainable parameters.
#model.layers[3].trainable = False
#model.summary()

"""## Checkpoint
Use the checkpoint callback to save a copy of the model weights after each epoch. This is especially handy on Google Colab because it likes to kick you off the vm in the middle of training when you're training large models.
"""

checkpoint_path = "/content/drive/My Drive/Sentiment/save_models/cp.ckpt"

# Create a callback that saves the model's weights
#cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
#                                                 save_weights_only=True,
#                                                 verbose=1)

"""## Optimizer, metrics and loss function
Set these up and compile the model
"""

# Set up epochs and steps
epochs = 3
batch_size = BS # Set this so that it fits on the GPU

train_data_size = len(x_train)
steps_per_epoch = int(train_data_size / batch_size)
num_train_steps = steps_per_epoch * epochs
warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)

## Creates an optimizer with learning rate schedule, using warmup steps and
## weight decay (AdamWeightDecay)
optimizer = nlp.optimization.create_optimizer(
    2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)


iterator = iter(train_ds)
ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=optimizer, model=model, iterator=iterator)
manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)

# Define our metrics
train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')
test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('test_accuracy')

test_auc = tf.keras.metrics.AUC(multi_label=True, dtype=tf.float32)

current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
train_log_dir = '/content/drive/My Drive/Sentiment/logs/gradient_tape/' + current_time + '/train'
test_log_dir = '/content/drive/My Drive/Sentiment/logs/gradient_tape/' + current_time + '/test'
train_summary_writer = tf.summary.create_file_writer(train_log_dir)
test_summary_writer = tf.summary.create_file_writer(test_log_dir)

tb_callback = tf.keras.callbacks.TensorBoard(train_log_dir)
tb_callback.set_model(model)

@tf.function(jit_compile=True)
def train_step(net, examples, labels, optimizer):
  """Trains `net` on `examples` using `optimizer`."""
  with tf.GradientTape() as tape:
    output = net(examples, training=True)
    loss = loss_object(labels, output)
  variables = net.trainable_variables
  gradients = tape.gradient(loss, variables)
  optimizer.apply_gradients(zip(gradients, variables))

  train_loss(loss)
  train_accuracy(labels, output)

  return loss

@tf.function(jit_compile=True)
def test_step(net, examples, labels):
  predictions = net(examples)
  loss = loss_object(labels, predictions)

  test_loss(loss)
  test_accuracy(labels, predictions)
  test_auc(tf.one_hot(labels, 3), predictions)

def write_tensorboard(step):
  with train_summary_writer.as_default():
    tf.summary.scalar('loss', train_loss.result(), step=step)
    tf.summary.scalar('accuracy', train_accuracy.result(), step=step)
    tf.summary.scalar('learning rate', optimizer._decayed_lr('float32'), step=step)
  with test_summary_writer.as_default():
    tf.summary.scalar('loss', test_loss.result(), step=step)
    tf.summary.scalar('accuracy', test_accuracy.result(), step=step)
    tf.summary.scalar('AUC', test_auc.result(), step=step)
  template = 'Step {} / {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}, AUC: {}'
  print(template.format(step,
                        num_train_steps+1,
                        train_loss.result(),
                        train_accuracy.result()*100,
                        test_loss.result(),
                        test_accuracy.result()*100,
                        test_auc.result()))
  train_loss.reset_states()
  test_loss.reset_states()
  train_accuracy.reset_states()
  test_accuracy.reset_states()
  test_auc.reset_states()

def train_and_checkpoint(net, manager):
  ckpt.restore(manager.latest_checkpoint)
  if manager.latest_checkpoint:
    print("Restored from {}".format(manager.latest_checkpoint))
  else:
    print("Initializing from scratch.")

  for _ in range(num_train_steps):
    examples, labels = next(iterator)
    loss = train_step(net, examples, labels, optimizer)
    ckpt.step.assign_add(1)
    if int(ckpt.step) % 5000 == 0:
      save_path = manager.save()
      print("Saved checkpoint for step {}: {}".format(int(ckpt.step), save_path))
      #print("loss {:1.2f}".format(loss.numpy()))
      for (x_test, y_test) in valid_ds:
        test_step(net, x_test, y_test)
      write_tensorboard(int(ckpt.step))

    if int(ckpt.step) >= num_train_steps:
      print('Finalizing training...')
      break

  for (x_test, y_test) in valid_ds:
    test_step(net, x_test, y_test)
  write_tensorboard(int(ckpt.step))

train_and_checkpoint(model, manager)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir="/content/drive/My Drive/Sentiment/logs/gradient_tape"

## Use sparse when the classes are not one hot encoded
#metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]
#loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False) # False when the output is a probability, like when using softmax

## Compile the model
#model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

"""## Train the model
Optionally, load the weights from a checkpoint. Use this when Colab kicked you off the vm and a checkpoint was saved.

## Get predictions and use them to evaluate the model
"""

p = model.predict(valid_ds)

auc = roc_auc_score(pd.get_dummies(y_valid).values, p, multi_class='ovo')
auc

cf_matrix = confusion_matrix(y_valid, p.argmax(axis=1))

plt.figure()
sn.heatmap(cf_matrix, annot=True)
plt.title('Confusion Matrix')
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

class_rep = classification_report(y_valid, p.argmax(axis=1))
print(class_rep)

"""## Discussion
We can see that the negative and neutral classes are not predicted very accurately. The model still achieves an accuracy of 84%, but this is mainly because the positive class has a high precision and recall and makes up about 77% of the validation set.

Some ideas to get better results:


1.   Get more data, like always.
2.   Try keeping the original 5 classes during training, and pool them into positive, negative and neutral after training and prediction.
3. I only trained for 2 epochs in this notebook. Try to train for more and implement an 'early stopping' callback.
4. Use BERT instead of distilBERT.
4. Train the model with stopwords removed from the reviews. This has already been implemented in the code above, but hasn't been used.


"""

